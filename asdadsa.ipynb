{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # !pip install sklearn-deap\n",
    "# # !pip install pycaret\n",
    "# # !pip install --force scikit-learn==1.0.2\n",
    "# # !pip install sklearn\n",
    "# # !pip install numpy\n",
    "# # # !pip uninstall -y sklearn\n",
    "# # # !pip uninstall -y scikit-learn\n",
    "# # !pip install --force numpy==1.19.5\n",
    "# # # !pip uninstall -y scikit-learn\n",
    "# # !pip install --force scikit-learn==0.23.2\n",
    "#\n",
    "# !pip uninstall --force -y numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# # !pip install sklearn\n",
    "# !pip install keras\n",
    "# # !pip install scikit-learn\n",
    "# !pip install scikeras\n",
    "#\n",
    "# # !pip uninstall -y numpy\n",
    "# #\n",
    "# # !pip install numpy==1.23.0\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "#\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print(\"Please install GPU version of TF\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_path = 'inp/IBIRITE (ROLA MOCA)_MG.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_path, sep=';')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.rename(columns={\"PRECIPITAÇÃO TOTAL, HORÁRIO (mm)\": \"Chuva\",\n",
    "                     \"PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)\": \"$Pressão_{Inst}$\",\n",
    "                     \"PRESSÃO ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB)\": \"$Pressão_{Max}$\",\n",
    "                     \"PRESSÃO ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB)\": \"$Pressão_{Min}$\",\n",
    "                     \"RADIACAO GLOBAL (KJ/m²)\": \"H\",\n",
    "                     \"TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)\": \"$Temperatura_{Inst}$\",\n",
    "                     \"TEMPERATURA DO PONTO DE ORVALHO (°C)\": \"$Orvalho_{Inst}$\",\n",
    "                     \"TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)\": \"$Temperatura_{Max}$\",\n",
    "                     \"TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)\": \"$Temperatura_{Min}$\",\n",
    "                     \"TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (°C)\": \"$Orvalho_{Max}$\",\n",
    "                     \"TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (°C)\": \"$Orvalho_{Min}$\",\n",
    "                     \"UMIDADE REL. MAX. NA HORA ANT. (AUT) (%)\": \"$Umidade_{Max}$\",\n",
    "                     \"UMIDADE REL. MIN. NA HORA ANT. (AUT) (%)\": \"$Umidade_{Min}$\",\n",
    "                     \"UMIDADE RELATIVA DO AR, HORARIA (%)\": \"$Umidade_{Inst}$\",\n",
    "                     \"VENTO, DIREÇÃO HORARIA (gr) (° (gr))\": \"$Vento_{Dir}$\",\n",
    "                     \"VENTO, RAJADA MAXIMA (m/s)\": \"$Vento_{Raj}$\",\n",
    "                     \"VENTO, VELOCIDADE HORARIA (m/s)\": \"$Vento_{Vel}$\",\n",
    "                     \"RADIACAO GLOBAL (Kj/m²)\": \"GLOBAL\"}, inplace=True)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.set_index('DATE', inplace=True)\n",
    "data.dropna(axis=0, how='all', subset=None, inplace=True)\n",
    "data.dropna(axis=1, how='all', subset=None, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DEPENDENDO DO ANO E DA ESTAÇÃO OS VALORES DE RADIAÇÃO ESTÃO EM COLUNAS DIFERENTES\n",
    "if 'GLOBAL' not in data.columns:\n",
    "    data.rename(columns={\"H\": \"GLOBAL\"}, inplace=True)\n",
    "if 'H' in data.columns:\n",
    "    data['GLOBAL'].fillna(data['H'], inplace=True)\n",
    "    data.drop(['H'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dates = pd.to_datetime(data.index)\n",
    "\n",
    "data = data[\n",
    "    ['$Temperatura_{Inst}$', '$Temperatura_{Max}$', '$Temperatura_{Min}$', '$Umidade_{Max}$', '$Umidade_{Min}$',\n",
    "     '$Umidade_{Inst}$', 'GLOBAL']]\n",
    "\n",
    "cols = list(data)[0:7]\n",
    "data = data[cols].astype(float)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GARANTINDO QUE TODOS OS DADOS ESTÃO SENDO TRATADOS COMO FLOAT\n",
    "data = data.replace(',', '.', regex=True)\n",
    "data['$Temperatura_{Inst}$'] = data['$Temperatura_{Inst}$'].astype(float)\n",
    "data['$Temperatura_{Max}$'] = data['$Temperatura_{Max}$'].astype(float)\n",
    "data['$Temperatura_{Min}$'] = data['$Temperatura_{Min}$'].astype(float)\n",
    "data['GLOBAL'] = data['GLOBAL'].astype(float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DELETANDO ALGUMA LEITURA ONDE O MÁXIMO É MENOR QUE O MÍNIMO\n",
    "delIndex = data.loc[(data['$Temperatura_{Max}$'] < data['$Temperatura_{Min}$']) | (\n",
    "        data['$Umidade_{Max}$'] < data['$Umidade_{Min}$'])].index\n",
    "if len(delIndex) > 0:\n",
    "    data.drop(delIndex, axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DELETANDO ALGUMA LEITURA ONDE O MÁXIMO É MENOR QUE O INSTANTÂNEO\n",
    "delIndex = data.loc[(data['$Temperatura_{Max}$'] < data['$Temperatura_{Inst}$']) | (\n",
    "        data['$Umidade_{Max}$'] < data['$Umidade_{Inst}$'])].index\n",
    "if len(delIndex) > 0:\n",
    "    data.drop(delIndex, axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DELETANDO ALGUMA LEITURA ONDE O INSTANTÂNEO É MENOR QUE O MÍNIMO\n",
    "delIndex = data.loc[(data['$Temperatura_{Inst}$'] < data['$Temperatura_{Min}$']) | (\n",
    "        data['$Umidade_{Inst}$'] < data['$Umidade_{Min}$'])].index\n",
    "if len(delIndex) > 0:\n",
    "    data.drop(delIndex, axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #MODIFICANDO A UNIDADE DA RADIAÇÃO DE (Kj/m²) PARA (Mj/m²)\n",
    "data['GLOBAL'] = data['GLOBAL'].mul(0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DELETANDO ALGUMA LINHA QUE AINDA TENHA NAN\n",
    "data.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EXTRAINDO OS NOMES DAS COLUNAS\n",
    "target_names = ['GLOBAL']\n",
    "variable_names = data.columns.values\n",
    "variable_names = np.delete(variable_names, np.where(variable_names == target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date_range = data.index\n",
    "\n",
    "var_to_plot = data.columns\n",
    "df = data[var_to_plot]\n",
    "\n",
    "n = int(df.shape[0] * 0.7)\n",
    "df.index = range(df.shape[0])\n",
    "id0 = df.index <= n\n",
    "id1 = df.index > n\n",
    "print(data.index)\n",
    "\n",
    "# pl.rc('text', usetex=False)\n",
    "# # pl.rc('font', family='serif',  serif='Times')\n",
    "# pl.rc('xtick', labelsize=20)\n",
    "# pl.rc('ytick', labelsize=20)\n",
    "# fig=pl.figure(figsize=(18,24))\n",
    "# for i,group in enumerate(data.columns):\n",
    "#     sub=pl.subplot(len(data.columns), 1, i+1)\n",
    "#     data[group].iloc[id0].plot( marker='', label='Training')#,fontsize=16,)#pyplot.plot(dataset[group].values)\n",
    "#     data[group].iloc[id1].plot( marker='', label='Test')#,fontsize=16,)#pyplot.plot(dataset[group].values)\n",
    "#     data[data.columns[i]].plot(marker='', lw=0)\n",
    "# #         pl.axvline(n, color='k', ls='-.')\n",
    "#     pl.ylabel(group,rotation = 0, fontsize=20,)\n",
    "#     sub.yaxis.tick_right()#COLOCANDO OS TICKS NA DIREITA\n",
    "#     sub.yaxis.set_label_coords(-0.05,0.5)#AJUSTANDO A POSIÇÃO DOS LABELS PARA NÃO SOBRESCREVER O PLOT\n",
    "# fig.autofmt_xdate(rotation=30)\n",
    "\n",
    "# pl.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# df = X.copy()\n",
    "\n",
    "# df[target_names] = y.values\n",
    "# corr = df.corr()\n",
    "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "# f, ax = pl.subplots(figsize=(11, 9))\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# cmap = cmap = \"YlGnBu\"\n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap,  #vmax=.3, center=0,\n",
    "#             annot=True,  #fmt=\"d\")\n",
    "#             square=True, linewidths=.5,  #cbar_kws={\"shrink\": .5},\n",
    "#             )\n",
    "# pl.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = data[variable_names]\n",
    "y = data[target_names]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# normalizing the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler = scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "n_future = 1\n",
    "n_past = 3\n",
    "\n",
    "for i in range(n_past, len(data) - n_future + 1):\n",
    "    train_x.append(data[i - n_past:i, 0:data.shape[1]])\n",
    "    train_y.append(data[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "# Separar os dados em conjuntos de treinamento e teste\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"Shape de train_x:\", train_x.shape)\n",
    "print(\"Shape de train_y:\", train_y.shape)\n",
    "print(\"Shape de test_x:\", test_x.shape)\n",
    "print(\"Shape de test_y:\", test_y.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # split the data for training and test, for training a want 70% of the data, and i need split the data in n_past_days=3 and n_future_days=1\n",
    "#\n",
    "# n_past = 3\n",
    "# n_future = 1\n",
    "#\n",
    "# total_samples = len(X)\n",
    "# n_samples = total_samples - (n_past - n_future) + 1\n",
    "#\n",
    "# train_size = int(n_samples * 0.7)\n",
    "# train_x, test_x = X[:train_size], X[train_size:]\n",
    "# train_y, test_y = y[:train_size], y[train_size:]\n",
    "#\n",
    "# # reshape\n",
    "# train_x = [train_x[i:i + n_past] for i in range(train_size - n_past + 1)]\n",
    "# test_x = [test_x[i:i + n_past] for i in range(len(test_x) - n_past + 1)]\n",
    "# train_y = train_y[n_past - 1:train_size + n_future - 1]\n",
    "# test_y = test_y[n_past - 1:]\n",
    "#\n",
    "# train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "# test_x, test_y = np.array(test_x), np.array(test_y)\n",
    "#\n",
    "# train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "# from tensorflow.keras.metrics import mean_squared_error, mean_absolute_error, RootMeanSquaredError\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "#\n",
    "# params = {\n",
    "#     'batch_size': [32],\n",
    "#     'epochs': [100],\n",
    "#     'optimizer': ['adam'],\n",
    "#     'learning_rate': [0.01],\n",
    "#     'activation': ['relu'],\n",
    "#     'dropout': [0.1],\n",
    "#     'neurons': [5]\n",
    "# }\n",
    "#\n",
    "#\n",
    "# def create_model(learning_rate=0.1, optimizer='adam', activation='relu', dropout=0.1, neurons=1):\n",
    "#     model1 = Sequential()\n",
    "#     model1.add(CuDNNLSTM(neurons, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=False))\n",
    "#     # model1.add(Dropout(dropout))\n",
    "#     model1.add(Dense(train_y.shape[1]))\n",
    "#     model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=learning_rate),\n",
    "#                    metrics=[RootMeanSquaredError()])\n",
    "#     return model1\n",
    "#\n",
    "#\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "#\n",
    "# model = KerasRegressor(model=create_model, verbose=2, learning_rate=[0.01], activation=['relu'],\n",
    "#                        dropout=[0.1], neurons=[5])\n",
    "#\n",
    "# # define checkpoint callback\n",
    "# checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#\n",
    "# eas = EvolutionaryAlgorithmSearchCV(estimator=model,\n",
    "#                                     params=params,\n",
    "#                                     scoring='neg_mean_squared_error',\n",
    "#                                     cv=3,\n",
    "#                                     verbose=1,\n",
    "#                                     population_size=50,\n",
    "#                                     gene_mutation_prob=0.01,\n",
    "#                                     gene_crossover_prob=0.8,\n",
    "#                                     tournament_size=3,\n",
    "#                                     generations_number=5)\n",
    "#\n",
    "# eas.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "\n",
    "params = {\n",
    "    'batch_size': [1024],\n",
    "    'epochs': [100, 200],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'activation': ['relu', 'sigmoid'],\n",
    "    'dropout': [0.2, 0.4],\n",
    "    'neurons': [32, 64]\n",
    "}\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.1, optimizer='adam', activation='relu', dropout=0.1, neurons=1):\n",
    "    model1 = Sequential()\n",
    "    model1.add(CuDNNLSTM(neurons, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=True))\n",
    "    model1.add(CuDNNLSTM(int(neurons/2), return_sequences=False))\n",
    "    model1.add(Dense(train_y.shape[1]))\n",
    "    model1.add(Dropout(dropout))\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    model1.compile(loss=MeanSquaredError(), optimizer=optimizer,\n",
    "                   metrics=[RootMeanSquaredError()])\n",
    "    return model1\n",
    "\n",
    "\n",
    "model = KerasRegressor(model=create_model, verbose=2, learning_rate=[0.01, 0.001], activation=['relu', 'sigmoid'],\n",
    "                       dropout=[0.2, 0.4], neurons=[32, 64])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "eas = EvolutionaryAlgorithmSearchCV(estimator=model,\n",
    "                                    params=params,\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    population_size=50,\n",
    "                                    gene_mutation_prob=0.01,\n",
    "                                    gene_crossover_prob=0.8,\n",
    "                                    tournament_size=3,\n",
    "                                    generations_number=15)\n",
    "\n",
    "# Enable graph execution mode\n",
    "tf.config.run_functions_eagerly(True)\n",
    "# tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Fit the model\n",
    "eas.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(eas.best_score_, eas.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = eas.predict(test_x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(test_y, predictions)\n",
    "r2 = r2_score(test_y, predictions)\n",
    "mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100\n",
    "print('MSE: %.3f, R2: %.3f' % (mse, r2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (720,3) (7,) (720,3) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# inverte a escala para os valores originais\u001B[39;00m\n\u001B[1;32m      2\u001B[0m predictions_copies \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrepeat(predictions, train_x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m original \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minverse_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions_copies\u001B[49m\u001B[43m)\u001B[49m[:,\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      4\u001B[0m original \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(original, (train_x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(original)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:529\u001B[0m, in \u001B[0;36mMinMaxScaler.inverse_transform\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    523\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    525\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[1;32m    526\u001B[0m     X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy, dtype\u001B[38;5;241m=\u001B[39mFLOAT_DTYPES, force_all_finite\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    527\u001B[0m )\n\u001B[0;32m--> 529\u001B[0m X \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_\n\u001B[1;32m    530\u001B[0m X \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (720,3) (7,) (720,3) "
     ]
    }
   ],
   "source": [
    "# inverte a escala para os valores originais\n",
    "predictions_copies = np.repeat(predictions, train_x.shape[1], axis=-1)\n",
    "original = scaler.inverse_transform(predictions_copies)[:,0]\n",
    "original = np.reshape(original, (train_x.shape[0], -1))\n",
    "print(original)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T20:22:46.305705300Z",
     "start_time": "2023-07-03T20:22:46.285793800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotar os resultados\n",
    "pl.figure(figsize=(10, 6))\n",
    "pl.plot(test_y, label='Valores Reais')\n",
    "pl.plot(predictions, label='Previsões')\n",
    "pl.xlabel('Índice')\n",
    "pl.ylabel('Valor')\n",
    "pl.title('Comparação entre Valores Reais e Previsões')\n",
    "pl.legend()\n",
    "pl.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
